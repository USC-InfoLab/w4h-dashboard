{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup database connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database=\"postgres\",\n",
    "                        host=\"localhost\",\n",
    "                        user=\"admin\",\n",
    "                        password=\"admin\",\n",
    "                        port=\"5432\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables():\n",
    "    \"\"\" Create tables in the PostgreSQL database\"\"\"\n",
    "    commands = (\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS heart_rates (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            user_id VARCHAR(50) NOT NULL,\n",
    "            email VARCHAR(50) NOT NULL,\n",
    "            date VARCHAR(50),\n",
    "            time VARCHAR(50),\n",
    "            heart_rate INTEGER\n",
    "        )\n",
    "        \"\"\",\n",
    "        \"\"\" CREATE TABLE IF NOT EXISTS calories (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                user_id VARCHAR(50) NOT NULL,\n",
    "                email VARCHAR(50) NOT NULL,\n",
    "                date VARCHAR(50),\n",
    "                time VARCHAR(50),\n",
    "                calories DOUBLE PRECISION\n",
    "                )\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS weights (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                user_id VARCHAR(50) NOT NULL,\n",
    "                email VARCHAR(50) NOT NULL,\n",
    "                date VARCHAR(50),\n",
    "                time VARCHAR(50),\n",
    "                weight_kg DOUBLE PRECISION\n",
    "        )\n",
    "        \"\"\")\n",
    "    try:\n",
    "\n",
    "        # execute the CREATE TABLE statement\n",
    "        for command in commands:\n",
    "            cursor.execute(command)\n",
    "    except (psycopg2.DatabaseError, Exception) as error:\n",
    "        print(error)\n",
    "\n",
    "create_tables()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1234\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate data range\n",
    "date_range = pd.date_range(start=\"2024-01-01\", end=\"2024-01-30\", freq = \"D\", closed=None)\n",
    "repeated_dates = pd.to_datetime(np.repeat(date_range, 5))\n",
    "\n",
    "new_time_samples = [ \"00:00:00\", \"08:00:00\", \"12:00:00\", \"16:00:00\", \"20:00:00\"]\n",
    "new_time_data = np.tile(new_time_samples, len(date_range))\n",
    "# new_time_date_data = [pd.to_datetime(str(date) + ' ' + str(time)) for date, time in zip(repeated_dates, new_time_data)]\n",
    "new_time_date_data = pd.to_datetime(repeated_dates) + pd.to_timedelta(new_time_data)\n",
    "# generate time\n",
    "time_samples = [\"20:41:00\", \"14:48:00\", \"14:47:00\", \"14:46:00\", \"14:45:00\", \"09:32:00\", \"04:48:00\", \"09:27:00\", \"02:21:00\", \"09:34:00\"]\n",
    "time_data = np.random.choice(time_samples, size=len(date_range))\n",
    "\n",
    "# heart rate\n",
    "heart_rate_data = np.random.randint(low=60, high=120, size=len(repeated_dates)).tolist()\n",
    "\n",
    "# calories\n",
    "calories_data = np.concatenate([\n",
    "    np.random.uniform(1, 2, size=int(len(repeated_dates) * 0.9)), \n",
    "    np.random.uniform(2, 9, size=int(len(repeated_dates) * 0.1)) \n",
    "])\n",
    "# make data disorder\n",
    "np.random.shuffle(calories_data)\n",
    "calories_data = calories_data.tolist()\n",
    "\n",
    "# weight\n",
    "weight_data = np.random.uniform(62, 78, size=len(repeated_dates)).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the synthetic data to the database..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rates_records =  [\n",
    "       (user_id, \"pps006@cancerbase.org\", date.strftime('%Y-%m-%d'), str(time), heart_rate) \n",
    "        for _, _, date, time, heart_rate in \n",
    "        zip(range(len(repeated_dates)), range(len(repeated_dates)), repeated_dates, new_time_data, heart_rate_data)\n",
    "    ]\n",
    "\n",
    "calories_records = [\n",
    "       (user_id, \"pps006@cancerbase.org\", date.strftime('%Y-%m-%d'), str(time), cal) \n",
    "        for _, _, date, time, cal in \n",
    "        zip(range(len(repeated_dates)), range(len(repeated_dates)), repeated_dates, new_time_data, calories_data)\n",
    "    ]\n",
    "\n",
    "weights_records = [\n",
    "       (user_id, \"pps006@cancerbase.org\", date.strftime('%Y-%m-%d'), str(time), weight) \n",
    "        for _, _, date, time, weight in \n",
    "        zip(range(len(repeated_dates)), range(len(repeated_dates)), repeated_dates, new_time_data, weight_data)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rates_records_sql = \"INSERT INTO heart_rates(user_id, email, date, time, heart_rate) VALUES(%s,%s,%s,%s,%s) \"\n",
    "calories_records_sql = \"INSERT INTO calories(user_id, email, date, time, calories) VALUES (%s,%s,%s,%s,%s)\"\n",
    "weights_records_sql = \"INSERT INTO weights(user_id, email, date, time, weight_kg) VALUES (%s,%s,%s,%s,%s)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # execute the INSERT statement\n",
    "    cursor.executemany(heart_rates_records_sql, heart_rates_records)\n",
    "    cursor.executemany(calories_records_sql, calories_records)\n",
    "    cursor.executemany(weights_records_sql, weights_records)\n",
    "    \n",
    "    # commit the changes to the database\n",
    "    conn.commit()\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up spark for mets calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/02 15:27:58 WARN Utils: Your hostname, node8 resolves to a loopback address: 127.0.1.1; using 192.168.0.27 instead (on interface eno1)\n",
      "24/06/02 15:27:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/06/02 15:27:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/02 15:27:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"demo\").config(\"spark.jars\", \"postgresql-42.7.2.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_df = spark.read.format(\"jdbc\"). \\\n",
    "options(\n",
    "         url='jdbc:postgresql://localhost:5432/postgres', # jdbc:postgresql://<host>:<port>/<database>\n",
    "         dbtable='heart_rates',\n",
    "         user='admin',\n",
    "         password='admin',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()\n",
    "\n",
    "calorie_df = spark.read.format(\"jdbc\"). \\\n",
    "options(\n",
    "         url='jdbc:postgresql://localhost:5432/postgres', # jdbc:postgresql://<host>:<port>/<database>\n",
    "         dbtable='calories',\n",
    "         user='admin',\n",
    "         password='admin',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()\n",
    "\n",
    "weight_df = spark.read.format(\"jdbc\"). \\\n",
    "options(\n",
    "         url='jdbc:postgresql://localhost:5432/postgres', # jdbc:postgresql://<host>:<port>/<database>\n",
    "         dbtable='weights',\n",
    "         user='admin',\n",
    "         password='admin',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculates Mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------------+-----------------+----------------+------------------+\n",
      "|      date|    time|heart_rate|         calories|        weight_kg|           joule|              mets|\n",
      "+----------+--------+----------+-----------------+-----------------+----------------+------------------+\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|65.86269792122519|4.59093553483232|1.2625364658819478|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 66.6143759573039|4.59093553483232|1.2482899775299414|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|74.53175101302101|4.59093553483232|1.1156863583197796|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|63.04486731442849|4.59093553483232|1.3189663395148945|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 77.8305564365509|4.59093553483232|1.0683986042770088|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|65.86269792122519|4.59093553483232|1.2625364658819478|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 66.6143759573039|4.59093553483232|1.2482899775299414|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|74.53175101302101|4.59093553483232|1.1156863583197796|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|63.04486731442849|4.59093553483232|1.3189663395148945|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 77.8305564365509|4.59093553483232|1.0683986042770088|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|65.86269792122519|4.59093553483232|1.2625364658819478|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 66.6143759573039|4.59093553483232|1.2482899775299414|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|74.53175101302101|4.59093553483232|1.1156863583197796|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|63.04486731442849|4.59093553483232|1.3189663395148945|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 77.8305564365509|4.59093553483232|1.0683986042770088|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|65.86269792122519|4.59093553483232|1.2625364658819478|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 66.6143759573039|4.59093553483232|1.2482899775299414|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|74.53175101302101|4.59093553483232|1.1156863583197796|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174|63.04486731442849|4.59093553483232|1.3189663395148945|\n",
      "|2024-01-16|12:00:00|        83|1.097259927063174| 77.8305564365509|4.59093553483232|1.0683986042770088|\n",
      "+----------+--------+----------+-----------------+-----------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_mets(heart_rate_df, calorie_df, weight_df):\n",
    "    merged_df = heart_rate_df.join(calorie_df, on=['date', 'time'], how='left').select('date', 'time', 'heart_rate', 'calories')  \n",
    "    \n",
    "    merged_df = merged_df.join(weight_df[['date', 'weight_kg']], on='date', how='left').select('date', 'time', 'heart_rate', 'calories','weight_kg')\n",
    "    \n",
    "    merged_df = merged_df.withColumn('joule', F.col('calories')* 4.184)\n",
    "    \n",
    "    merged_df = merged_df.withColumn('mets', (F.col('joule')/F.col('weight_kg')))\n",
    "    \n",
    "    # if(merged_df.shape[0] == 0):\n",
    "    #     print('no merged data')\n",
    "    #     return merged_df\n",
    "    \n",
    "    mode = 0.055210 # for the sake of consistency between pandas version\n",
    "    # mode = merged_df.groupby('mets').count().orderBy('count', ascending=True).first()[0]\n",
    "    times = 1.00 /mode\n",
    "    merged_df = merged_df.withColumn('mets', F.col('mets') * times)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "merged_df = get_mets(heart_rate_df, calorie_df,weight_df)\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Active hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|      date|     fairly_active|    lightly_active|         sedentary|       very_active|      total_active|     non-sedentary|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|2024-01-19|               0.0|17.066666666666666|               9.6|               0.0|26.666666666666664|17.066666666666666|\n",
      "|2024-01-13|               0.0|21.333333333333332| 5.333333333333333|               0.0|26.666666666666664|21.333333333333332|\n",
      "|2024-01-06| 5.333333333333333|              16.0| 5.333333333333333|               0.0|26.666666666666664|21.333333333333332|\n",
      "|2024-01-14|2.1333333333333333|14.933333333333334|1.0666666666666667| 8.533333333333333|26.666666666666664|              25.6|\n",
      "|2024-01-10|               0.0|              22.4| 4.266666666666667|               0.0|26.666666666666664|              22.4|\n",
      "|2024-01-08|               0.0|14.933333333333334|11.733333333333333|               0.0|26.666666666666664|14.933333333333334|\n",
      "|2024-01-17| 5.333333333333333| 7.466666666666667|13.866666666666667|               0.0|26.666666666666668|              12.8|\n",
      "|2024-01-02|               0.0|              25.6|1.0666666666666667|               0.0|26.666666666666668|              25.6|\n",
      "|2024-01-18|               0.0|18.133333333333333| 8.533333333333333|               0.0|26.666666666666664|18.133333333333333|\n",
      "|2024-01-16|               0.0|14.933333333333334|11.733333333333333|               0.0|26.666666666666664|14.933333333333334|\n",
      "|2024-01-12|               0.0|23.466666666666665|               3.2|               0.0|26.666666666666664|23.466666666666665|\n",
      "|2024-01-29|               0.0| 5.333333333333333|21.333333333333332|               0.0|26.666666666666664| 5.333333333333333|\n",
      "|2024-01-15|               0.0|11.733333333333333|14.933333333333334|               0.0|26.666666666666664|11.733333333333333|\n",
      "|2024-01-04|               0.0| 5.333333333333333|10.666666666666666|10.666666666666666|26.666666666666664|              16.0|\n",
      "|2024-01-30|               0.0|              16.0|10.666666666666666|               0.0|26.666666666666664|              16.0|\n",
      "|2024-01-03| 5.333333333333333|21.333333333333332|               0.0|               0.0|26.666666666666664|26.666666666666664|\n",
      "|2024-01-25|               0.0|10.666666666666666|              16.0|               0.0|26.666666666666664|10.666666666666666|\n",
      "|2024-01-22|               0.0|13.866666666666667| 7.466666666666667| 5.333333333333333|26.666666666666668|              19.2|\n",
      "|2024-01-27| 5.333333333333333|10.666666666666666| 5.333333333333333| 5.333333333333333|26.666666666666664|21.333333333333332|\n",
      "|2024-01-05|               0.0|20.266666666666666|1.0666666666666667| 5.333333333333333|26.666666666666664|25.599999999999998|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize_mets(merged_df):\n",
    "    def divide_mets(mets):\n",
    "        if mets < 1.5:\n",
    "            return 'sedentary'\n",
    "        elif 1.5 <= mets < 3.0:\n",
    "            return 'lightly_active'\n",
    "        elif 3.0 <= mets < 6.0:\n",
    "            return 'fairly_active'\n",
    "        else:\n",
    "            return 'very_active'\n",
    "            \n",
    "    divide_mets_udf = F.udf(lambda x:divide_mets(x), StringType()) \n",
    "    \n",
    "    merged_df =  merged_df.withColumn('mets_category', divide_mets_udf(F.col('mets')))\n",
    "\n",
    "    # categorized by date and level\n",
    "    mets_df = merged_df.groupby('date', 'mets_category').pivot('mets_category').agg((F.count('time')/60)).fillna(0) \n",
    "    \n",
    "    if 'sedentary' not in mets_df.columns:\n",
    "        mets_df = mets_df.withColumn('sedentary', F.lit(0)) \n",
    "    if 'lightly_active' not in mets_df.columns:\n",
    "        mets_df = mets_df.withColumn('lightly_active', F.lit(0)) \n",
    "    if 'fairly_active' not in mets_df.columns:\n",
    "        mets_df = mets_df.withColumn('fairly_active', F.lit(0)) \n",
    "    if 'very_active' not in mets_df.columns:\n",
    "        mets_df = mets_df.withColumn('very_active', F.lit(0)) \n",
    "    \n",
    "    mets_df.drop('mets_category')\n",
    "    mets_df = mets_df.groupby('date').agg(F.sum('fairly_active').alias('fairly_active'),\n",
    "                                           F.sum('lightly_active').alias('lightly_active'),\n",
    "                                           F.sum('sedentary').alias('sedentary'),\n",
    "                                           F.sum('very_active').alias('very_active'))      \n",
    "    \n",
    "    mets_df = mets_df.withColumn('total_active', F.col('sedentary') + F.col('lightly_active') + F.col('fairly_active') + F.col('very_active'))\n",
    "    mets_df = mets_df.withColumn('non-sedentary', F.col('lightly_active') + F.col('fairly_active') + F.col('very_active'))    \n",
    "    \n",
    "    return mets_df\n",
    "mets_df = categorize_mets(merged_df)\n",
    "mets_df.show()\n",
    "mets_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
